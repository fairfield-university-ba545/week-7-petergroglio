{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering in Python with scikit-learn\n",
    "Learn about the inner workings of the K-Means clustering algorithm with an interesting case study.\n",
    "\n",
    "In Machine Learning, the types of Learning can broadly be classified into three types: 1. Supervised Learning, 2. Unsupervised Learning and 3. Semi-supervised Learning. Algorithms belonging to the family of Unsupervised Learning have no variable to predict tied to the data. Instead of having an output, the data only has an input which would be multiple variables that describe the data. This is where clustering comes in.\n",
    "\n",
    "Clustering is the task of grouping together a set of objects in a way that objects in the same cluster are more similar to each other than to objects in other clusters. Similarity is a metric that reflects the strength of relationship between two data objects. Clustering is mainly used for exploratory data mining. It has manifold usage in many fields such as machine learning, pattern recognition, image analysis, information retrieval, bio-informatics, data compression, and computer graphics.\n",
    "\n",
    "However, this post tries to unravel the inner workings of K-Means, a very popular clustering technique. There's also a very good [DataCamp post](https://www.datacamp.com/community/tutorials/k-means-clustering-r) on K-Means, which explains the types of clustering (hard and soft clustering), types of clustering methods (connectivity, centroid, distribution and density) with a case study. The algorithm will help you to tackle unlabeled datasets (i.e. the datasets that do not have any class-labels) and draw your own inferences from them with ease.\n",
    "\n",
    "K-Means falls under the category of centroid-based clustering. A centroid is a data point (imaginary or real) at the center of a cluster. In centroid-based clustering, clusters are represented by a central vector or a centroid. This centroid might not necessarily be a member of the dataset. Centroid-based clustering is an iterative algorithm in which the notion of similarity is derived by how close a data point is to the centroid of the cluster.\n",
    "\n",
    "In this post, you will learn about:\n",
    "- The inner workings of the K-Means algorithm\n",
    "- A simple case study in Python\n",
    "- The disadvantages of K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The inner workings of the K-Means clustering algorithm:\n",
    "To do this, you will need a sample dataset (training set):\n",
    "\n",
    "|Objects|X|Y|Z|\n",
    "|-------|-|-|-|\n",
    "|OB-1   |1|4|1|\n",
    "|OB-2   |1|2|2|\n",
    "|OB-3   |1|4|2|\n",
    "|OB-4   |2|1|2|\n",
    "|OB-5   |1|1|1|\n",
    "|OB-6   |2|4|2|\n",
    "|OB-7   |1|1|2|\n",
    "|OB-8   |2|1|1|\n",
    "\n",
    "The sample dataset contains 8 objects with their X, Y and Z coordinates. Your task is to cluster these objects into two clusters (here you define the value of K (of K-Means) in essence to be 2).\n",
    "\n",
    "So, the algorithm works by:\n",
    "\n",
    "- Taking any two centroids or data points (as you took 2 as K hence the number of centroids also 2) in its account initially.\n",
    "- After choosing the centroids, (say C1 and C2) the data points (coordinates here) are assigned to any of the Clusters (let’s take centroids = clusters for the time being) depending upon the distance between them and the centroids.\n",
    "- Assume that the algorithm chose OB-2 (1,2,2) and OB-6 (2,4,2) as centroids and cluster 1 and cluster 2 as well.\n",
    "- For measuring the distances, you take the following distance measurement function (also termed as similarity measurement function):\n",
    "\n",
    "$$ d=|x2–x1|+|y2–y1|+|z2–z1| $$\n",
    "        \n",
    "This is also known as the Taxicab distance or Manhattan distance, where d is distance measurement between two objects, (x1,y1,z1) and (x2,y2,z2) are the X, Y and Z coordinates of any two objects taken for distance measurement.\n",
    "\n",
    "Feel free to check out other distance measurement functions like Euclidean Distance, Cosine Distance etc.\n",
    "\n",
    "The following table shows the calculation of distances (using the above distance measurement function) between the objects and centroids (OB-2 and OB-6): \n",
    "\n",
    "|Objects|X|Y|Z|Distance from C1(1,2,2)|Distance from C2(2,4,2)|\n",
    "|-------|-|-|-|-----------------------|-----------------------|\n",
    "|OB-1\t|1|4|1|3\t                  |2|\n",
    "|OB-2\t|1|2|2|0\t                  |3|\n",
    "|OB-3\t|1|4|2|2\t                  |1|\n",
    "|OB-4\t|2|1|2|2\t                  |3|\n",
    "|OB-5\t|1|1|1|2\t                  |5|\n",
    "|OB-6\t|2|4|2|3\t                  |0|\n",
    "|OB-7\t|1|1|2|1\t                  |4|\n",
    "|OB-8\t|2|1|1|3                \t  |4|\n",
    "\n",
    "The objects are clustered based on their distances between the centroids. An object which has a shorter distance between a centroid (say C1) than the other centroid (say C2) will fall into the cluster of C1. After the initial pass of clustering, the clustered objects will look something like the following:\n",
    "\n",
    "|Cluster 1|Cluster 2|\n",
    "|---------|---------|\n",
    "|OB-2     |OB-1|\n",
    "|OB-4     |OB-3|\n",
    "|OB-5     |OB-6|\n",
    "|OB-7     | |\n",
    "|OB-8     | |\n",
    "\n",
    "Now the algorithm will continue updating cluster centroids (i.e the coordinates) until they cannot be updated anymore (more on when it cannot be updated later). The updation takes place in the following manner:\n",
    "\n",
    "\\begin{equation*}\n",
    "(\\frac{\\sum_{i-1}^n x_i}{n}),\n",
    "(\\frac{\\sum_{i-1}^n y_i}{n}),\n",
    "(\\frac{\\sum_{i-1}^n z_i}{n})\n",
    "\\end{equation*}\n",
    "\n",
    "where $n$ = number of objects belonging to that particular cluster.\n",
    "\n",
    "So, following this rule the updated cluster 1 will be ((1+2+1+1+2)/5, (2+1+1+1+1)/5,(2+2+1+2+1)/5) = (1.4,1.2,1.6). And for cluster 2 it will be ((1+1+2)/3, (4+4+4)/3, (1+2+2)/3) = (1.33, 4, 1.66).\n",
    "\n",
    "After this, the algorithm again starts finding the distances between the data points and newly derived cluster centroids. So the new distances will be like following: \n",
    "\n",
    "|Objects|X|Y|Z|Distance from C1(1.4,1.2,1.6)|Distance from C2(1.33,4,1.66)|\n",
    "|-------|-|-|-|-----------------------|-----------------------|\n",
    "|OB-1\t|1|4|1|3.8\t                  |1|\n",
    "|OB-2\t|1|2|2|1.6\t                  |2.66|\n",
    "|OB-3\t|1|4|2|3.6\t                  |0.66|\n",
    "|OB-4\t|2|1|2|1.2\t                  |4|\n",
    "|OB-5\t|1|1|1|1.2\t                  |4|\n",
    "|OB-6\t|2|4|2|3.8\t                  |1|\n",
    "|OB-7\t|1|1|2|1\t                  |3.66|\n",
    "|OB-8\t|2|1|1|1.4                \t  |4.33|\n",
    "\n",
    "The new assignments of the objects with respect to the updated clusters will be: \n",
    "\n",
    "|Cluster 1|Cluster 2|\n",
    "|---------|---------|\n",
    "|OB-2     |OB-1|\n",
    "|OB-4     |OB-3|\n",
    "|OB-5     |OB-6|\n",
    "|OB-7     | |\n",
    "|OB-8     | |\n",
    "\n",
    "This is where the algorithm no longer updates the centroids. Because there is no change in the current cluster formation, it is the same as the previous formation.\n",
    "\n",
    "Now when, you are done with the cluster formation with K-Means you may apply it to some data the algorithm has not seen before (what you call a Test set). Let's generate that: \n",
    "\n",
    "|Objects|X|Y|Z|\n",
    "|-------|-|-|-|\n",
    "|OB-1   |2|4|1|\n",
    "|OB-2   |2|2|2|\n",
    "|OB-3   |1|2|1|\n",
    "|OB-4   |2|2|1|\n",
    "\n",
    "After applying K-means on the above dataset, the final clusters will be:\n",
    "\n",
    "|Cluster 1|Cluster 2|\n",
    "|---------|---------|\n",
    "|OB-2     |OB-1|\n",
    "|OB-3     | |\n",
    "|OB-4     | |\n",
    "\n",
    "Any application of an algorithm is incomplete if one is not sure about its performance. Now, in order to know how well the K-Means algorithm is performing there are certain metrics to consider. Some of these metrics are:\n",
    "\n",
    "- Adjusted rand index\n",
    "- Mutual information based scoring\n",
    "- Homogeneity, completeness and v-measure\n",
    "\n",
    "Now that you have got familiar with the inner mechanics of K-Means let's see K-Means live in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple case study of K-Means in Python:\n",
    "For the implementation part, you will be using the Titanic dataset (available here). Before proceeding with it, I would like to discuss some facts about the data itself. The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "Now, talking about the dataset, the training set contains several records about the passengers of Titanic (hence the name of the dataset). It has 12 features capturing information about `passenger_class`, `port_of_Embarkation`, `passenger_fare` etc. The dataset's label is survival which denotes the survivial status of a particular passenger. Your task is to cluster the records into two i.e. the ones who survived and the ones who did not.\n",
    "\n",
    "You might be thinking that since it is a labeled dataset, how could it be used for a clustering task? You just have to drop the 'survival' column from the dataset and make it unlabeled. It's the task of K-Means to cluster the records of the datasets if they survived or not.\n",
    "\n",
    "For this tutorial, you will need the following Python packages: `pandas`, `NumPy`, `scikit-learn`, `Seaborn` and `Matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have imported all the dependencies that you will need in this tutorial. Now, you will load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the train and test datasets to create two DataFrames\n",
    "\n",
    "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "train = pd.read_csv(train_url)\n",
    "test_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\n",
    "test = pd.read_csv(test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview the kind of data you will be working with by printing some samples from both the train and test DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"***** Train_Set *****\")\n",
    "print(train.head())\n",
    "print(\"\\n\")\n",
    "print(\"***** Test_Set *****\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get some initial statistics of both the train and test DataFrames using pandas' `describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"***** Train_Set *****\")\n",
    "print(train.describe())\n",
    "print(\"\\n\")\n",
    "print(\"***** Test_Set *****\")\n",
    "print(test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, from the above outputs you definitely got to know about the features of the dataset and some basic statistics of it. I will list the feature names for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to note that not all machine learning algorithms support missing values in the data that you are feeding to them. K-Means being one of them. So we need to handle the missing values present in the data. Let's first see where are the values missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For the train set\n",
    "train.isna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For the test set\n",
    "test.isna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the total number of missing values in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"*****In the train set*****\")\n",
    "print(train.isna().sum())\n",
    "print(\"\\n\")\n",
    "print(\"*****In the test set*****\")\n",
    "print(test.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can see in the training set, in the columns Age, Cabin and Embarked, there are missing values and in the test set, the Age and Cabin columns contain missing values.\n",
    "\n",
    "There are a couple of ways to handle missing values:\n",
    "\n",
    "- Remove rows with missing values\n",
    "- Impute missing values\n",
    "\n",
    "I prefer the latter one because if you remove the rows with missing values it can cause insufficiency in the data which in turn results in inefficient training of the machine learning model.\n",
    "\n",
    "Now, there are several ways you can perform the imputation:\n",
    "\n",
    "- A constant value that has meaning within the domain, such as 0, distinct from all other values.\n",
    "- A value from another randomly selected record.\n",
    "- A mean, median or mode value for the column.\n",
    "- A value estimated by another machine learning model.\n",
    "\n",
    "Any imputation performed on the train set will have to be performed on test data in the future when predictions are needed from the final machine learning model. This needs to be taken into consideration when choosing how to impute the missing values.\n",
    "\n",
    "Pandas provides the `fillna()` function for replacing missing values with a specific value. Let's apply that with Mean Imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values with mean column values in the train set\n",
    "train.fillna(train.mean(), inplace=True)\n",
    "\n",
    "# Fill missing values with mean column values in the test set\n",
    "test.fillna(test.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have imputed the missing values in the dataset, it's time to see if the dataset still has any missing values.\n",
    "\n",
    "For the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you have any missing values in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you can see there are still some missing values in the Cabin and Embarked columns. This is because these values are non-numeric. In order to perform the imputation the values need to be in numeric form. There are ways to convert a non-numeric value to a numeric one. More on this later.\n",
    "\n",
    "Let's do some more analytics in order to understand the data better. Understanding is really required in order to perform any Machine Learning task. Let's start with finding out which features are categorical and which are numerical.\n",
    "\n",
    "- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\n",
    "- Continuous: Age, Fare. Discrete: SibSp, Parch.\n",
    "\n",
    "Two features are left out which are not listed above in any of the categories. Yes, you guessed it right, Ticket and Cabin. Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric. Let see some sample values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['Ticket'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['Cabin'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the survival count of passengers with respect to the following features:\n",
    "\n",
    "- Pclass\n",
    "- Sex\n",
    "- SibSp\n",
    "- Parch\n",
    "- Let's do that one by one:\n",
    "\n",
    "Survival count with respect to Pclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survival count with respect to Sex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the survival rate of female passengers is significantly higher for males.\n",
    "\n",
    "Survival count with respect to SibSp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for some quick plotting. Let's first plot the graph of \"Age vs. Survived\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(train, col='Survived')\n",
    "g.map(plt.hist, 'Age', bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its time to see how the Pclass and Survived features are related to eachother with a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\n",
    "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough of visualization and analytics for now! Let's actually build a K-Means model with the training set. But before that you will need some data preprocessing as well. You can see that not all the feature values are of same type. Some of them are numerical and some of them are not. In order to ease the computation, you will feed all numerical data to the model. Let's see the data types of different features that you have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can see that the following features are non-numeric:\n",
    "\n",
    "- Name\n",
    "- Sex\n",
    "- Ticket\n",
    "- Cabin\n",
    "- Embarked\n",
    "\n",
    "Before converting them into numeric ones, you might want to do some feature engineering, i.e. features like Name, Ticket, Cabin and Embarked do not have any impact on the survival status of the passengers. Often, it is better to train your model with only significant features than to train it with all the features, including unnecessary ones. It not only helps in efficient modelling, but also the training of the model can happen in much lesser time. Although, feature engineering is a whole field of study itself, I will encourage you to dig it further. But for this tutorial, know that the features Name, Ticket, Cabin and Embarked can be dropped and they will not have significant impact on the training of the K-Means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['Name','Ticket', 'Cabin','Embarked'], axis=1)\n",
    "test = test.drop(['Name','Ticket', 'Cabin','Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dropping part is done let's convert the 'Sex' feature to a numerical one (only 'Sex' is remaining now which is a non-numeric feature). You will do this using a technique called [Label Encoding](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder.fit(train['Sex'])\n",
    "labelEncoder.fit(test['Sex'])\n",
    "train['Sex'] = labelEncoder.transform(train['Sex'])\n",
    "test['Sex'] = labelEncoder.transform(test['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's investigate if you have non-numeric data left\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the test set does not have the Survived feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brilliant!**\n",
    "\n",
    "Looks like you are good to go to train your K-Means model now.\n",
    "\n",
    "You can first drop the Survival column from the data with the `drop()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(train.drop(['Survived'], 1).astype(float))\n",
    "\n",
    "y = np.array(train['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can review all the features you are going to feed to the algorithm with `train.info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build the K-Means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2) # You want cluster the passenger records into 2: Survived or Not survived\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see all the other parameters of the model other than `n_clusters`. Let's see how well the model is doing by looking at the percentage of passenger records that were clustered correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(X)):\n",
    "    predict_me = np.array(X[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = kmeans.predict(predict_me)\n",
    "    if prediction[0] == y[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct/len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is nice for the first go. Your model was able to cluster correctly with a 50% (accuracy of your model). But in order to enhance the performance of the model you could tweak some parameters of the model itself. I will list some of these parameters which the scikit-learn implementation of K-Means provides:\n",
    "\n",
    "- algorithm\n",
    "- max_iter\n",
    "- n_jobs \n",
    "\n",
    "Let's tweak the values of these parameters and see if there is a change in the result.\n",
    "\n",
    "In the [scikit-learn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), you will find a solid information about these parameters which you should dig further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = kmeans = KMeans(n_clusters=2, max_iter=600, algorithm = 'auto')\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(X)):\n",
    "    predict_me = np.array(X[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = kmeans.predict(predict_me)\n",
    "    if prediction[0] == y[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct/len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a decrease in the score. One of the reasons being you have not scaled the values of the different features that you are feeding to the model. The features in the dataset contain different ranges of values. So, what happens is a small change in a feature does not affect the other feature. So, it is also important to scale the values of the features to a same range.\n",
    "\n",
    "Let's do that now and for this experiment you are going to take `[0,1]` as the uniform value range across all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(X)):\n",
    "    predict_me = np.array(X[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = kmeans.predict(predict_me)\n",
    "    if prediction[0] == y[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct/len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You can see an instant 12% increase in the score.\n",
    "\n",
    "So far you were able to load your data, preprocess it accordingly, do a little bit of feature engineering and finally you were able to make a K-Means model and see it in action.\n",
    "\n",
    "Now, let's discuss K-Means's limitations.\n",
    "\n",
    "\n",
    "## Disadvantages of K-Means\n",
    "Now that you have a fairly good idea on how K-Means algorithm works let's discuss some its disadvantages.\n",
    "\n",
    "The biggest disadvantage is that K-Means requires you to pre-specify the number of clusters (k). However, for the Titanic dataset, you had some domain knowledge available that told you the number of people who survived in the shipwreck. This might not always be the case with real world datasets. Hierarchical clustering is an alternative approach that does not require a particular choice of clusters. An additional disadvantage of k-means is that it is sensitive to outliers and different results can occur if you change the ordering of the data.\n",
    "\n",
    "K-Means is a lazy learner where generalization of the training data is delayed until a query is made to the system. This means K-Means starts working only when you trigger it to, thus lazy learning methods can construct a different approximation or result to the target function for each encountered query. It is a good method for online learning, but it requires a possibly large amount of memory to store the data, and each request involves starting the identification of a local model from scratch.\n",
    "\n",
    "## Conclusion\n",
    "So, in this tutorial you scratched the surface of one of the most popular clustering techniques - K-Means. You learned about its inner mechanics, implemented it using the Titanic Dataset in Python, and you also got a fair idea of its disadvantages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
