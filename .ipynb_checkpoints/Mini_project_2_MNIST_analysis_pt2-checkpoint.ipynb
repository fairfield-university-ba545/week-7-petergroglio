{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project #2: MNIST Analysis\n",
    "\n",
    "An easy-to-follow scikit-learn tutorial that will help you to get started with the Python machine learning.\n",
    "\n",
    "## Machine Learning with Python\n",
    "\n",
    "Machine learning is a branch in computer science that studies the design of algorithms that can learn.\n",
    "\n",
    "Typical tasks are concept learning, function learning or “predictive modeling”, clustering and finding predictive patterns. These tasks are learned through available data that were observed through experiences or instructions, for example.\n",
    "\n",
    "The hope that comes with this discipline is that including the experience into its tasks will eventually improve the learning. But this improvement needs to happen in such a way that the learning itself becomes automatic so that humans like ourselves don’t need to interfere anymore is the ultimate goal.\n",
    "\n",
    "Today’s scikit-learn tutorial will introduce you to the basics of Python machine learning:\n",
    "\n",
    "- Part 1: You'll learn how to use Python and its libraries to explore your data with the help of matplotlib and Principal Component Analysis (PCA),\n",
    "- Part 2a: And you'll preprocess your data with normalization and you'll split your data into training and test sets.\n",
    "- Part 2b: Next, you'll work with the well-known KMeans algorithm to construct an unsupervised model, fit this model to your data, predict values, and validate the model that you have built.\n",
    "- Part 3: As an extra, you'll also see how you can also use Support Vector Machines (SVM) to construct another model to classify your data.\n",
    "\n",
    "Let's move to part 2 now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "\n",
    "Now that you have even more information about your data and you have a visualization ready, it does seem a bit like the data points sort of group together, but you also see there is quite some overlap.\n",
    "\n",
    "This might be interesting to investigate further.\n",
    "\n",
    "Do you think that, in a case where you knew that there are 10 possible digits labels to assign to the data points, but you have no access to the labels, the observations would group or “cluster” together by some criterion in such a way that you could infer the labels?\n",
    "\n",
    "Now this is an analytical question called __unsupervised learning__.\n",
    "\n",
    "In general, when you have acquired a good understanding of your data, you have to decide on the use cases that would be relevant to your data set. In other words, you think about what your data set might teach you or what you think you can learn from your data.\n",
    "\n",
    "From there on, you can think about what kind of algorithms you would be able to apply to your data set in order to get the results that you think you can obtain.\n",
    "\n",
    "Tip: the more familiar you are with your data, the easier it will be to assess the use cases for your specific data set. The same also holds for finding the appropriate machine algorithm.\n",
    "\n",
    "However, when you’re first getting started with `scikit-learn`, you’ll see that the amount of algorithms that the library contains is pretty vast and that you might still want additional help when you’re doing the assessment for your data set. That’s why the below `scikit-learn` machine learning map will come in handy.\n",
    "\n",
    "<img src = 'https://scikit-learn.org/stable/_static/ml_map.png' />\n",
    "[Image Source](https://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "\n",
    "Note that this map does require you to have some knowledge about the algorithms that are included in the `scikit-learn `library. This, by the way, also holds some truth for taking this next step in your project: if you have no idea what is possible, it will be very hard to decide on what your use case will be for the data.\n",
    "\n",
    "As your use case was one for clustering, you can follow the path on the map towards “KMeans”. You’ll see the use case that you have just thought about requires you to have more than 50 samples (“check!”), to have labeled data (“check!”), to know the number of categories that you want to predict (“check!”) and to have less than 10K samples (“check!”).\n",
    "\n",
    "But what exactly is the __K-Means__ algorithm?\n",
    "\n",
    "It is one of the simplest and widely used __unsupervised learning__ algorithms to solve __clustering__ problems. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters that you have set before you run the algorithm. This number of clusters is called `k` and you select this number at your discretion.\n",
    "\n",
    "Then, the k-means algorithm will find the __nearest cluster center__ for each data point and assign the closest data point to that cluster.\n",
    "\n",
    "Once all data points have been assigned to clusters, the cluster centers will be recomputed. In other words, new cluster centers will emerge from the average of the values of the cluster data points. This process is repeated until most data points stick to the same cluster. The cluster membership should stabilize.\n",
    "\n",
    "You can already see that, because the k-means algorithm works the way it does, the initial set of cluster centers that you give up can have a __big effect__ on the clusters that are eventually found. You can, of course, deal with this effect, as you will see further on.\n",
    "\n",
    "However, before you can go into making a model for your data, you should definitely take a look into preparing your data for this purpose. \n",
    "\n",
    "Let's first import the `digits` data from `scikit-learn` `datasets` again. If you don't remember how to do so, please refer to part 1 of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import `datasets` from `sklearn`.\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# read `digits` using `load_digits()` into a variable called `digits`\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Your Data\n",
    "As you have read in the previous section, before modeling your data, you’ll do well by preparing it first. This preparation step is called __preprocessing__.\n",
    "\n",
    "### Data Normalization\n",
    "The first thing that we’re going to do is normalize/standardize the data. \n",
    "\n",
    "We often use these two together, but they actually mean different things:\n",
    "\n",
    "- __Normalization__ usually means making the data values in the same range;\n",
    "- __Standardization__ usually means making the data values follow the standardized distribution (mean of `0` and standard deviation of `1`).\n",
    "\n",
    "You can standardize the digits data by, for example, making use of the `scale()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scale() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-58782be04200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Apply `scale()` to the `digits` data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdigits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: scale() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "# Import `scale` from `sklearn.preprocessing`\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Apply `scale()` to the `digits` data\n",
    "digits = scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By scaling the data, you shift the distribution of each attribute to have a mean of `0` and a standard deviation of `1` (unit variance).\n",
    "\n",
    "### Splitting Your Data Into Training And Test Sets\n",
    "In order to assess your model’s performance later, you will also need to divide the data set into two parts: a __training set__ and a __test set__. The first is used to train the model, while the second is used to evaluate the learned or trained model.\n",
    "\n",
    "__NOTE__: in part 1 if you stick with the `pandas` `read_csv()` method to get your data, it is already split into __training__ and __test__.\n",
    "\n",
    "In practice, the division of your data set into a test and a training sets is disjoint: the most common splitting choice is to take `2/3` of your original data set as the training set, while the `1/3` that remains will compose the test set.\n",
    "\n",
    "You will try to do this  here. You see in the code chunk below that this ‘traditional’ splitting choice is respected: in the arguments of the `train_test_split()` method, you clearly see that the test_size is set to 0.25 - which means 75% of the data will be used in training the model.\n",
    "\n",
    "You’ll also note that the argument `random_state` has the value `2019` assigned to it. With this argument, you can guarantee that your split will always be the same. That is particularly handy if you want reproducible results.\n",
    "\n",
    "__NOTE__: you should always use `random_state`, or its equivalent parameters (e.g. `seed`), in your analysis, to guarantee the reproducibility and comparison capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `train_test_split` from `sklearn.model_selection`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the `digits` data into training and test sets\n",
    "# Split `data` into `X_train` and `X_test`\n",
    "# Split `target` into `y_train` and `y_test`\n",
    "# Split `images` into `images_train` and `images_test`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have split up your data set into train and test sets, you can quickly inspect the numbers before you go and model the data. You should look at the dimensionalities of your `X` and `y` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of training features\n",
    "n_samples, n_features = X_train.shape\n",
    "\n",
    "# Print out `n_samples`\n",
    "#### add your comments here explaining what is `n_samples`\n",
    "print(n_samples)\n",
    "\n",
    "# Print out `n_features`\n",
    "#### add your comments here explaining what is `n_features`\n",
    "print(n_features)\n",
    "\n",
    "# Number of Training labels\n",
    "n_digits = len(np.unique(y_train))\n",
    "\n",
    "# Inspect `y_train`\n",
    "#### add your comment here explaining what is the value below\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll see that the training set `X_train` now contains `1347` samples, which is exactly `75%` of the samples that the original data set contained, and 64 features, which hasn’t changed. The y_train training set also contains `75%` of the labels of the original data set. This means that the test sets `X_test` and `y_test` contain `450` samples.\n",
    "\n",
    "## Clustering The `digits` Data\n",
    "After all these preparation steps, you have made sure that all your known (training) data is stored. No actual model or learning was performed up until this moment.\n",
    "\n",
    "Now, it’s finally time to find those clusters of your training set. Use KMeans() from the cluster module to set up your model. You’ll see that there are three arguments that are passed to this method: `init`, `n_clusters` and the `random_state`.\n",
    "\n",
    "You might still remember this last argument from before when you split the data into training and test sets. This argument basically guaranteed that you got reproducible results. Please use the same `random_state` here.\n",
    "\n",
    "The other important thing here is determining the value of `k` in `n_clusters`. Here it is easy (`n_clusters = 10`) since we already know that the data has `10` classes. There is a greater chance we you conduct an __unsupervised learning__ analysis and you do not have classes - then you will have to try different value of `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the `cluster` module\n",
    "\n",
    "\n",
    "# Create the KMeans model\n",
    "clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=2019)\n",
    "\n",
    "# Fit the training data `X_train`to the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `init` parameter indicates the method for initialization and even though it defaults to `k-means++`, you see it explicitly coming back in the code. That means that you can leave it out if you want. \n",
    "\n",
    "Next, you also see that the `n_clusters` argument is set to 10. This number not only indicates the number of clusters or groups you want your data to form, but also the number of centroids to generate. _Remember that a cluster centroid is the __center__ of a cluster_.\n",
    "\n",
    "Do you also still remember how the previous section described this as one of the possible disadvantages of the K-Means algorithm?\n",
    "\n",
    "That is, that the initial set of cluster centers that you give up can have a big effect on the clusters that are eventually found?\n",
    "\n",
    "Usually, you try to deal with this effect by trying __several__ initial sets in multiple runs and by selecting the set of clusters with the __minimum__ _sum of the squared errors (SSE)_. In other words, you want to minimize the distance of each point in the cluster to the mean or centroid of that cluster.\n",
    "\n",
    "By adding the `n-init` argument to `KMeans()`, you can determine how many different centroid configurations the algorithm will try.\n",
    "\n",
    "Note again that you don’t want to insert the test labels when you fit the model to your data: these will be used to see if your model is good at predicting the actual classes of your instances!\n",
    "\n",
    "You can also visualize the images that make up the cluster centers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Figure size in inches\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "# Add title\n",
    "fig.suptitle('Cluster Center Images', fontsize=14, fontweight='bold')\n",
    "\n",
    "# For all labels (0-9)\n",
    "for i in range(10):\n",
    "    # Initialize subplots in a grid of 2X5, at i+1th position\n",
    "    ax = fig.add_subplot(2, 5, 1 + i)\n",
    "    # Display images\n",
    "    ax.imshow(clf.cluster_centers_[i].reshape((8, 8)), cmap=plt.cm.binary)\n",
    "    # Don't show the axes\n",
    "    plt.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these images are blurry, correct? That's because they are averages of the points in different clusters. From there you can already guess which clusters will perform better than others. For instance, the right-most one on the first row is obviously an `0` - which means all images in that cluster are similar.\n",
    "\n",
    "Yes - you want all data points in one cluster to be similar; and you want data points between different clusters to be as different as they can be - that is the idea behind a very important evaluation metric, the `silhouette score`.\n",
    "\n",
    "If you want to see another example that visualizes the data clusters and their centers, go here.\n",
    "\n",
    "The next step is to apply the clustering (results from a cluster model) to the __test data__, to generate labels for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the labels for `X_test` using `.predict()`\n",
    "\n",
    "\n",
    "# Print out the first 100 instances of `y_pred`\n",
    "\n",
    "\n",
    "# Print out the first 100 instances of `y_test`\n",
    "\n",
    "\n",
    "# Study the shape of the cluster centers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use following code to determine how different `y_pred` and `y_test` are - that is how we evaluate the predictive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare `y_test` with `y_pred`\n",
    "np.count_nonzero(y_test == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means only `38` out of `100` samples are correct.\n",
    "\n",
    "In addition, you can study the shape of the cluster centers: you immediately see that there are `10` clusters with each `64` features.\n",
    "\n",
    "But this doesn’t tell you much because we set the number of clusters to `10` and you already knew that there were `64` features.\n",
    "\n",
    "Maybe a visualization would be more helpful.\n",
    "\n",
    "Let’s visualize the predicted labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `Isomap()` from `sklearn.manifold`\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "# Create an isomap and fit the `digits` data to it\n",
    "X_iso = Isomap(n_neighbors=10).fit_transform(X_train)\n",
    "\n",
    "# Compute cluster centers and predict cluster index for each sample\n",
    "clusters = clf.fit_predict(X_train)\n",
    "\n",
    "# Create a plot with subplots in a grid of 1X2\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Adjust layout\n",
    "fig.suptitle('Predicted Versus Training Labels', fontsize=14, fontweight='bold')\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "# Add scatterplots to the subplots \n",
    "ax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=clusters)\n",
    "ax[0].set_title('Predicted Training Labels')\n",
    "ax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=y_train)\n",
    "ax[1].set_title('Actual Training Labels')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You use `Isomap()` as a way to reduce the dimensions of your high-dimensional data set digits. The difference with the PCA method is that the Isomap is a __non-linear__ dimensionality reduction method.\n",
    "\n",
    "We will create another dimensionality reduction using PCA. Please complete the code below - if you do not remember how to create PCA models, please refer to part 1 of this mini project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `PCA` from `sklearn.decomposition`\n",
    "\n",
    "\n",
    "# We will use 2 principal components, and the PCA model will fit (`fit_transform()`) to `X_train`\n",
    "# and name the model `X_pca`\n",
    "\n",
    "\n",
    "# Compute cluster centers and predict cluster index for each sample\n",
    "# fit (`fit_predict`) `clf` to `X_train` and name it as `clusters`\n",
    "\n",
    "\n",
    "# Create a plot with subplots in a grid of 1X2\n",
    "# check the code block above if you have doubts about this\n",
    "\n",
    "# Adjust layout\n",
    "fig.suptitle('Predicted Versus Training Labels', fontsize=14, fontweight='bold')\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "# Add scatterplots to the subplots\n",
    "# in `ax[0]`, x-axis is the first component of `X_pca`, y-axis is the second component of `X_pca`\n",
    "# and color (`c`) is `clusters`\n",
    "# which means `ax[0]` depicts our prediction\n",
    "\n",
    "# you will set the title of `ax[0]` as 'Predicted Training Labels'\n",
    "\n",
    "# we also need to create another subplot `ax[1]` to depict the actual data `y_train`\n",
    "# in `ax[1]`, x-axis is the first component of `X_pca`, y-axis is the second component of `X_pca`\n",
    "\n",
    "# you will set the title of `ax[10]` as 'Actual Training Labels'\n",
    "\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight, the visualization doesn’t seem to indicate whether the model works well.\n",
    "\n",
    "But this needs some further investigation.\n",
    "\n",
    "### Evaluation of Your Clustering Model\n",
    "And this need for further investigation brings you to the next essential step, which is the evaluation of your model’s performance. In other words, you want to analyze the degree of correctness of the model’s predictions - in classification analysis, we use __confusion matrix__ for that.\n",
    "\n",
    "Let’s print out a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `confusion_matrix` from `sklearn.metrics`\n",
    "\n",
    "\n",
    "# Print out the confusion matrix with `confusion_matrix()`\n",
    "# the `confusion_matrix()` should be between `y_test` and `y_pred`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will want higher values to appear in the top-left to lower-right diagonal line of the matrix.\n",
    "\n",
    "### Answer the question:\n",
    "\n",
    "Observe the confusion matrix above - which digit(s) are predicted well? Use the block below to provide your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Double click and type your answer here__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to know a bit more about the results than just the confusion matrix.\n",
    "\n",
    "Let’s try to figure out something more about the quality of the clusters by applying different cluster quality metrics. That way, you can judge the goodness of fit of the cluster labels to the correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n",
    "print('% 9s' % 'inertia homo    compl   v-meas  ARI     AMI      silhouette')\n",
    "print('%i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n",
    "          %(clf.inertia_,\n",
    "      homogeneity_score(y_test, y_pred),\n",
    "      completeness_score(y_test, y_pred),\n",
    "      v_measure_score(y_test, y_pred),\n",
    "      adjusted_rand_score(y_test, y_pred),\n",
    "      adjusted_mutual_info_score(y_test, y_pred),\n",
    "      silhouette_score(X_test, y_pred, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll see that there are quite some metrics to consider:\n",
    "\n",
    "- The __homogeneity__ (`homo`) score tells you to what extent all of the clusters contain only data points which are members of a single class.\n",
    "- The __completeness__ (`compl`) score measures the extent to which all of the data points that are members of a given class are also elements of the same cluster - e.g. how many `0`s in cluster `0`.\n",
    "- The __V-measure__ (`v-meas`) score is the harmonic mean between homogeneity and completeness.\n",
    "- The adjusted Rand index (`ARI`) measures the similarity between two clusterings and considers all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "- The Adjusted Mutual Info (`AMI`) score is used to compare clusters. It measures the similarity between the data points that are in the clusterings, accounting for chance groupings and takes a maximum value of 1 when clusterings are equivalent.\n",
    "- The `silhouette` score measures how similar an object is to its own cluster compared to other clusters. The silhouette scores ranges from -1 to 1, where a higher value indicates that the object is better matched to its own cluster and worse mached to neighboring clusters. If many points have a high value, the clusteirng configuration is good.\n",
    "\n",
    "You clearly see that these scores aren’t fantastic: for example, you see that the value for the `silhouette` score is close to `0`, which indicates that the sample is on or very close to the decision boundary between two neighboring clusters. This could indicate that the samples could have been assigned to the wrong cluster.\n",
    "\n",
    "Also the `ARI` measure seems to indicate that not all data points in a given cluster are similar and the `completeness` score tells you that there are definitely data points that weren’t put in the right cluster.\n",
    "\n",
    "Looks like we did not do too well using __unsupervised, cluster__ models, in the next part we will try a __supervised learning__ model to see if the results will improve.\n",
    "\n",
    "That's all for part 2. Please make sure your sync the complete notebook to your github repo for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
